{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvMRbVLEJlZT"
   },
   "outputs": [],
   "source": [
    "#@title ü§ó AutoTrain DreamBooth\n",
    "#@markdown In order to use this colab\n",
    "#@markdown - upload images to a folder named `images/`\n",
    "#@markdown - choose a project name if you wish\n",
    "#@markdown - change model if you wish, you can also select sd2/2.1 or sd1.5\n",
    "#@markdown - update prompt and remember it. choose keywords that don't usually appear in dictionaries\n",
    "#@markdown - add huggingface information (token) if you wish to push trained model to huggingface hub\n",
    "#@markdown - update hyperparameters if you wish\n",
    "#@markdown - click `Runtime > Run all` or run each cell individually\n",
    "#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n",
    "\n",
    "import os\n",
    "!pip install -U autotrain-advanced > install_logs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6cM0erkraVP",
    "outputId": "7faabd4b-01d8-4d7f-a38e-7e02c699f56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2-_lkBS1WKA"
   },
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #### Project Config\n",
    "project_name = 'LoRA-finetune-muddy' # @param {type:\"string\"}\n",
    "model_name = 'stabilityai/stable-diffusion-xl-base-1.0' # @param [\"stabilityai/stable-diffusion-xl-base-1.0\", \"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\", \"stabilityai/stable-diffusion-2-1-base\"]\n",
    "prompt = 'muddy underwater style' # @param {type: \"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Push to Hub?\n",
    "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
    "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
    "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
    "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
    "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "hf_token = \"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\" #@param {type:\"string\"}\n",
    "hf_username = \"backpropagatorsiitg\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown #### Hyperparameters\n",
    "learning_rate = 1e-4 # @param {type:\"number\"}\n",
    "num_steps = 500 #@param {type:\"number\"}\n",
    "batch_size = 1 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "gradient_accumulation = 4 # @param {type:\"slider\", min:1, max:32, step:1}\n",
    "resolution = 1024 # @param {type:\"slider\", min:128, max:1024, step:128}\n",
    "use_8bit_adam = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "use_xformers = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n",
    "train_text_encoder = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "disable_gradient_checkpointing = False # @param [\"False\", \"True\"] {type:\"raw\"}\n",
    "\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PROMPT\"] = prompt\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_STEPS\"] = str(num_steps)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"RESOLUTION\"] = str(resolution)\n",
    "os.environ[\"USE_8BIT_ADAM\"] = str(use_8bit_adam)\n",
    "os.environ[\"USE_XFORMERS\"] = str(use_xformers)\n",
    "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
    "os.environ[\"TRAIN_TEXT_ENCODER\"] = str(train_text_encoder)\n",
    "os.environ[\"DISABLE_GRADIENT_CHECKPOINTING\"] = str(disable_gradient_checkpointing)\n",
    "os.environ[\"HF_USERNAME\"] = hf_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2OjNPe2F-bA",
    "outputId": "326f2a88-c32d-48d2-b5f8-0992d04bd096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub==0.23.2\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.2) (2024.8.30)\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/401.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.0\n",
      "    Uninstalling huggingface-hub-0.23.0:\n",
      "      Successfully uninstalled huggingface-hub-0.23.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autotrain-advanced 0.8.29 requires huggingface-hub==0.26.2, but you have huggingface-hub 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.23.2\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "g3cd_ED_yXXt",
    "outputId": "ae3fe65c-838a-4b43-daff-889e4f8ddf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.cli.run_dreambooth\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m388\u001b[0m - \u001b[1mRunning DreamBooth Training\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m286\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: train_split, inference, data_path, config, deploy, backend, valid_split, train, func, version, log\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/8.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/7.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/10.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/3.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/1.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/6.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/5.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/4.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/9.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mSaving concept images\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.preprocessor.dreambooth\u001b[0m:\u001b[36m_save_concept_images\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mimages/2.jpg\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m523\u001b[0m - \u001b[1m['python', '-m', 'autotrain.trainers.dreambooth', '--training_config', 'LoRA-finetune-muddy/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:47:41\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m524\u001b[0m - \u001b[1m{'model': 'stabilityai/stable-diffusion-xl-base-1.0', 'vae_model': None, 'revision': None, 'tokenizer': None, 'image_path': 'LoRA-finetune-muddy/autotrain-data', 'class_image_path': None, 'prompt': 'muddy underwater style', 'class_prompt': None, 'num_class_images': 100, 'class_labels_conditioning': None, 'prior_preservation': False, 'prior_loss_weight': 1.0, 'project_name': 'LoRA-finetune-muddy', 'seed': 42, 'resolution': 1024, 'center_crop': False, 'train_text_encoder': False, 'batch_size': 1, 'sample_batch_size': 4, 'epochs': 1, 'num_steps': 500, 'checkpointing_steps': 100000, 'resume_from_checkpoint': None, 'gradient_accumulation': 4, 'disable_gradient_checkpointing': False, 'lr': 0.0001, 'scale_lr': False, 'scheduler': 'constant', 'warmup_steps': 0, 'num_cycles': 1, 'lr_power': 1.0, 'dataloader_num_workers': 0, 'use_8bit_adam': False, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'allow_tf32': False, 'prior_generation_precision': None, 'local_rank': -1, 'xformers': False, 'pre_compute_text_embeddings': False, 'tokenizer_max_length': None, 'text_encoder_use_attention_mask': False, 'rank': 4, 'xl': True, 'mixed_precision': 'fp16', 'token': '*****', 'push_to_hub': True, 'username': 'backpropagatorsiitg', 'validation_prompt': None, 'num_validation_images': 4, 'validation_epochs': 50, 'checkpoints_total_limit': None, 'validation_images': None, 'logging': False}\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-11-08 18:47:50\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[33m\u001b[1mFailed to download dataset: 404 Client Error. (Request ID: Root=1-672e5cd6-75b6173f1031c7bd131ef820;75a446ac-7c39-4fc6-821f-374ab2446c82)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/datasets/LoRA-finetune-muddy/autotrain-data/revision/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\u001b[0m\n",
      "tokenizer/tokenizer_config.json: 100% 737/737 [00:00<00:00, 5.44MB/s]\n",
      "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 3.22MB/s]\n",
      "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 2.13MB/s]\n",
      "tokenizer/special_tokens_map.json: 100% 472/472 [00:00<00:00, 2.98MB/s]\n",
      "tokenizer_2/tokenizer_config.json: 100% 725/725 [00:00<00:00, 4.30MB/s]\n",
      "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 2.84MB/s]\n",
      "text_encoder/config.json: 100% 565/565 [00:00<00:00, 3.41MB/s]\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "text_encoder_2/config.json: 100% 575/575 [00:00<00:00, 4.72MB/s]\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "model_index.json: 100% 609/609 [00:00<00:00, 4.76MB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "scheduler/scheduler_config.json: 100% 479/479 [00:00<00:00, 2.85MB/s]\n",
      "{'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'thresholding', 'variance_type', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "model.safetensors: 100% 492M/492M [00:02<00:00, 213MB/s]\n",
      "model.safetensors: 100% 2.78G/2.78G [00:15<00:00, 175MB/s]\n",
      "vae/config.json: 100% 642/642 [00:00<00:00, 3.85MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 238MB/s]\n",
      "{'latents_std', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "unet/config.json: 100% 1.68k/1.68k [00:00<00:00, 10.4MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 10.3G/10.3G [01:02<00:00, 164MB/s]\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m931\u001b[0m - \u001b[1m***** Running training *****\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m932\u001b[0m - \u001b[1m  Num examples = 10\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m933\u001b[0m - \u001b[1m  Num batches each epoch = 10\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m934\u001b[0m - \u001b[1m  Num Epochs = 167\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m935\u001b[0m - \u001b[1m  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m936\u001b[0m - \u001b[1m  Total train batch size (w. parallel, distributed & accumulation) = 4\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m937\u001b[0m - \u001b[1m  Gradient Accumulation steps = 4\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 18:50:36\u001b[0m | \u001b[36mautotrain.trainers.dreambooth.train_xl\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m938\u001b[0m - \u001b[1m  Total optimization steps = 500\u001b[0m\n",
      "Steps: 100% 500/500 [1:20:39<00:00, 10.30s/it, loss=0.161, lr=0.0001]  Model weights saved in LoRA-finetune-muddy/pytorch_lora_weights.safetensors\n",
      "Steps: 100% 500/500 [1:20:39<00:00,  9.68s/it, loss=0.161, lr=0.0001]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 20:11:16\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mConverting model to Kohya format...\u001b[0m\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/23.4M [00:00<?, ?B/s]\n",
      "Upload 2 LFS files:   0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:   6% 1.38M/23.4M [00:00<00:01, 13.0MB/s]\n",
      "\n",
      "pytorch_lora_weights.safetensors:  34% 7.91M/23.4M [00:00<00:00, 21.4MB/s]\n",
      "\n",
      "pytorch_lora_weights_kohya.safetensors:  41% 9.54M/23.5M [00:00<00:00, 17.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_lora_weights.safetensors:  59% 13.8M/23.4M [00:00<00:00, 16.5MB/s]\n",
      "\n",
      "pytorch_lora_weights.safetensors:  96% 22.5M/23.4M [00:01<00:00, 17.9MB/s]\n",
      "\n",
      "pytorch_lora_weights_kohya.safetensors: 100% 23.5M/23.5M [00:02<00:00, 10.6MB/s]\n",
      "pytorch_lora_weights.safetensors: 100% 23.4M/23.4M [00:02<00:00, 10.2MB/s]\n",
      "\n",
      "Upload 2 LFS files: 100% 2/2 [00:02<00:00,  1.26s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-11-08 20:11:22\u001b[0m | \u001b[36mautotrain.cli.run_dreambooth\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m392\u001b[0m - \u001b[1mJob ID: 5268\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain dreambooth \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--image-path images/ \\\n",
    "--prompt \"${PROMPT}\" \\\n",
    "--resolution ${RESOLUTION} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--num-steps ${NUM_STEPS} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--mixed-precision ${MIXED_PRECISION} \\\n",
    "--username ${HF_USERNAME} \\\n",
    "$( [[ \"$USE_XFORMERS\" == \"True\" ]] && echo \"--xformers\" ) \\\n",
    "$( [[ \"$TRAIN_TEXT_ENCODER\" == \"True\" ]] && echo \"--train-text-encoder\" ) \\\n",
    "$( [[ \"$USE_8BIT_ADAM\" == \"True\" ]] && echo \"--use-8bit-adam\" ) \\\n",
    "$( [[ \"$DISABLE_GRADIENT_CHECKPOINTING\" == \"True\" ]] && echo \"--disable_gradient-checkpointing\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN}\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
